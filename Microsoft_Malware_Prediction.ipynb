{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venigarcia/Microsoft-Malware-Prediction/blob/main/Microsoft_Malware_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cNHfk26AqIJ"
      },
      "source": [
        "**Projeto de Introdução à Ciência de Dados (INF2420)**\n",
        "\n",
        "Integrantes: Carlos Vinicios, Eduardo Nascimento, Venícius Garcia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USoF05ex_2U_"
      },
      "source": [
        "## Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCYbA39lBIHa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FyQcAhRpH8y"
      },
      "outputs": [],
      "source": [
        "!pip install qgrid\n",
        "!pip install pyod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCTNVD9XCe9C"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlNNqGyNp5Sk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import qgrid, warnings, os, gc\n",
        "\n",
        "#feature engenier\n",
        "from datetime import datetime, date, timedelta\n",
        "\n",
        "#encoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "#outliers\n",
        "from scipy.stats import kurtosis, skew\n",
        "from pyod.models.knn import KNN\n",
        "from pyod.models.hbos import HBOS\n",
        "from pyod.utils.data import evaluate_print\n",
        "from joblib import dump, load\n",
        "\n",
        "#Feature Selection\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import pickle as pk\n",
        "\n",
        "#classificadores\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import lightgbm as lgb\n",
        "import joblib, pickle\n",
        "from operator import itemgetter\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, hp, space_eval\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DATA_ENV = \"/content/drive/MyDrive/Carlos/Dados/dados\"\n",
        "MODELS_ENV = \"/content/drive/MyDrive/Carlos/Dados/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vEpfR6Yqdqm"
      },
      "outputs": [],
      "source": [
        "#Permite ao Pandas alocar somente o espaço necessário na memória\n",
        "dtypes = {\n",
        "    'MachineIdentifier':                                    'category',\n",
        "    'ProductName':                                          'category',\n",
        "    'EngineVersion':                                        'category',\n",
        "    'AppVersion':                                           'category',\n",
        "    'AvSigVersion':                                         'category',\n",
        "    'IsBeta':                                               'int8',\n",
        "    'RtpStateBitfield':                                     'float16',\n",
        "    'IsSxsPassiveMode':                                     'int8',\n",
        "    'DefaultBrowsersIdentifier':                            'float16',\n",
        "    'AVProductStatesIdentifier':                            'float32',\n",
        "    'AVProductsInstalled':                                  'float16',\n",
        "    'AVProductsEnabled':                                    'float16',\n",
        "    'HasTpm':                                               'int8',\n",
        "    'CountryIdentifier':                                    'int16',\n",
        "    'CityIdentifier':                                       'float32',\n",
        "    'OrganizationIdentifier':                               'float16',\n",
        "    'GeoNameIdentifier':                                    'float16',\n",
        "    'LocaleEnglishNameIdentifier':                          'int8',\n",
        "    'Platform':                                             'category',\n",
        "    'Processor':                                            'category',\n",
        "    'OsVer':                                                'category',\n",
        "    'OsBuild':                                              'int16',\n",
        "    'OsSuite':                                              'int16',\n",
        "    'OsPlatformSubRelease':                                 'category',\n",
        "    'OsBuildLab':                                           'category',\n",
        "    'SkuEdition':                                           'category',\n",
        "    'IsProtected':                                          'float16',\n",
        "    'AutoSampleOptIn':                                      'int8',\n",
        "    'PuaMode':                                              'category',\n",
        "    'SMode':                                                'float16',\n",
        "    'IeVerIdentifier':                                      'float16',\n",
        "    'SmartScreen':                                          'category',\n",
        "    'Firewall':                                             'float16',\n",
        "    'UacLuaenable':                                         'float32',\n",
        "    'Census_MDC2FormFactor':                                'category',\n",
        "    'Census_DeviceFamily':                                  'category',\n",
        "    'Census_OEMNameIdentifier':                             'float16',\n",
        "    'Census_OEMModelIdentifier':                            'float32',\n",
        "    'Census_ProcessorCoreCount':                            'float16',\n",
        "    'Census_ProcessorManufacturerIdentifier':               'float16',\n",
        "    'Census_ProcessorModelIdentifier':                      'float16',\n",
        "    'Census_ProcessorClass':                                'category',\n",
        "    'Census_PrimaryDiskTotalCapacity':                      'float32',\n",
        "    'Census_PrimaryDiskTypeName':                           'category',\n",
        "    'Census_SystemVolumeTotalCapacity':                     'float32',\n",
        "    'Census_HasOpticalDiskDrive':                           'int8',\n",
        "    'Census_TotalPhysicalRAM':                              'float32',\n",
        "    'Census_ChassisTypeName':                               'category',\n",
        "    'Census_InternalPrimaryDiagonalDisplaySizeInInches':    'float16',\n",
        "    'Census_InternalPrimaryDisplayResolutionHorizontal':    'float16',\n",
        "    'Census_InternalPrimaryDisplayResolutionVertical':      'float16',\n",
        "    'Census_PowerPlatformRoleName':                         'category',\n",
        "    'Census_InternalBatteryType':                           'category',\n",
        "    'Census_InternalBatteryNumberOfCharges':                'float32',\n",
        "    'Census_OSVersion':                                     'category',\n",
        "    'Census_OSArchitecture':                                'category',\n",
        "    'Census_OSBranch':                                      'category',\n",
        "    'Census_OSBuildNumber':                                 'int16',\n",
        "    'Census_OSBuildRevision':                               'int32',\n",
        "    'Census_OSEdition':                                     'category',\n",
        "    'Census_OSSkuName':                                     'category',\n",
        "    'Census_OSInstallTypeName':                             'category',\n",
        "    'Census_OSInstallLanguageIdentifier':                   'float16',\n",
        "    'Census_OSUILocaleIdentifier':                          'int16',\n",
        "    'Census_OSWUAutoUpdateOptionsName':                     'category',\n",
        "    'Census_IsPortableOperatingSystem':                     'int8',\n",
        "    'Census_GenuineStateName':                              'category',\n",
        "    'Census_ActivationChannel':                             'category',\n",
        "    'Census_IsFlightingInternal':                           'float16',\n",
        "    'Census_IsFlightsDisabled':                             'float16',\n",
        "    'Census_FlightRing':                                    'category',\n",
        "    'Census_ThresholdOptIn':                                'float16',\n",
        "    'Census_FirmwareManufacturerIdentifier':                'float16',\n",
        "    'Census_FirmwareVersionIdentifier':                     'float32',\n",
        "    'Census_IsSecureBootEnabled':                           'int8',\n",
        "    'Census_IsWIMBootEnabled':                              'float16',\n",
        "    'Census_IsVirtualDevice':                               'float16',\n",
        "    'Census_IsTouchEnabled':                                'int8',\n",
        "    'Census_IsPenCapable':                                  'int8',\n",
        "    'Census_IsAlwaysOnAlwaysConnectedCapable':              'float16',\n",
        "    'Wdft_IsGamer':                                         'float16',\n",
        "    'Wdft_RegionIdentifier':                                'float16',\n",
        "    'HasDetections':                                        'int8'\n",
        "}\n",
        "\n",
        "# definindo um treshold para remover colunas que apresentam uma % de NA superior\n",
        "na_rate_threshold = 0.9\n",
        "\n",
        "# theshold to remove columns with unbalanced features to their values\n",
        "unbalanced_feature_rate_threshold = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMB8slX0qtPl"
      },
      "outputs": [],
      "source": [
        "#Leitura da base de dados com os dados de treino\n",
        "data_train = pd.read_csv(os.path.join(DATA_ENV, 'train.csv'), dtype=dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv7IR6CXsufR"
      },
      "outputs": [],
      "source": [
        "#Leitura da base de dados com os dados de teste\n",
        "data_test = pd.read_csv(os.path.join(DATA_ENV, 'test.csv'), dtype=dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_IkQfpf7ly4"
      },
      "source": [
        "## Análise Exploratória"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMFJayOOAHuN"
      },
      "source": [
        "A feature alvo que nós desejamos é saber se o computador está infectado ou não, que está demarcado na coluna HasDetections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A73PX-1GfKw"
      },
      "outputs": [],
      "source": [
        "data_train['HasDetections'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR1mGt40AHuO"
      },
      "source": [
        "Verificando os valores nulos e únicos, presentes na base de dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76Qm0x_HrrGR"
      },
      "outputs": [],
      "source": [
        "stats = []\n",
        "for col in data.columns:\n",
        "    stats.append((col, data[col].dtype, data[col].nunique(), data[col].isnull().sum() * 100 / data.shape[0], data[col].value_counts(normalize=True, dropna=False).values[0] * 100))\n",
        "    \n",
        "stats_df = pd.DataFrame(stats, columns=['Feature', \"Tipo\", 'Valores Únicos', '% de valores ausentes', '% dos valores únicos'])\n",
        "stats_df = stats_df.sort_values('% de valores ausentes', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVERZK7X_a3J"
      },
      "outputs": [],
      "source": [
        "stats_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5cP9szoAHuP"
      },
      "source": [
        "Verificamos também que a base encontra-se balanceada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-IWzj4j7qUP"
      },
      "outputs": [],
      "source": [
        "sns.set_theme(style=\"whitegrid\")\n",
        "fig, ax = plt.subplots()\n",
        "ax.ticklabel_format(style='plain')\n",
        "ax = sns.barplot([\"Não Detectado\", \"Detectado\"], data_train['HasDetections'].value_counts().to_list())\n",
        "plt.title('Classe Alvo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6ZodxxwAHuP"
      },
      "source": [
        "Os computadores possuem tpm desde 2005, logo existem mais amostras com tpm. Portanto não é conclusivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjVa6qkcMj9Z"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.ticklabel_format(style='plain')\n",
        "ax = sns.countplot(x='HasTpm', hue='HasDetections',data=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9aP12VGvPZ4"
      },
      "source": [
        "Infecções por arquitetura do processador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M60WrtZPMSdv"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.ticklabel_format(style='plain')\n",
        "ax = sns.countplot(x='Processor', hue='HasDetections',data=data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oItvAjqnoa9"
      },
      "source": [
        "Census_ProcessorClass - Uma classificação dos processadores em high/medium/low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHQeMqygnglJ"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x='Census_ProcessorClass', hue='HasDetections',data=data)\n",
        "plt.title('Census_ProcessorClass')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDoiNytCtkXW"
      },
      "source": [
        "IsProtected é uma váriavel booleana:\n",
        "* TRUE: se existe pelo menos um antivirus instalado e atualizado rodando na máquina. \n",
        "* FALSE: se não existe antivirus ativo na máquina ou se está ativo mas não recebe mais atualizações.\n",
        "\n",
        "PuaMode é referente a proteção contra PUAs(Potentially Unwanted Applications), aplicações que vem juntas à outras aplicações como um pacote. Entretanto, apresenta uma alta taxa de valores nulos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfltuPf8qmHB"
      },
      "outputs": [],
      "source": [
        "data['PuaMode'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiUIngIDAHuR"
      },
      "source": [
        "Verificando as infeções com base na localização (país) em que a máquina se encontra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOV2Ki0oxIcd"
      },
      "outputs": [],
      "source": [
        "value_counts = data['CountryIdentifier'].value_counts(dropna=True, sort=True)\n",
        "df_val_counts = pd.DataFrame(value_counts)\n",
        "df_value_counts_reset = df_val_counts.reset_index()\n",
        "df_value_counts_reset.columns = ['unique_values', 'counts']\n",
        "new_data = data.loc[data['CountryIdentifier'].isin(df_value_counts_reset.unique_values[:20])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_fyUeUJx4RS",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "fig.suptitle('Quantidade infecções por País')\n",
        "sns.barplot(ax=ax[0], x='unique_values',y='counts',data=df_value_counts_reset[:20])\n",
        "sns.countplot(ax=ax[1],x='CountryIdentifier', hue='HasDetections',data=new_data)\n",
        "ax[0].set_title(\"Quantidade de amostras por país\")\n",
        "ax[1].set_title(\"Infecção por país\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkpJi_LaAHuS"
      },
      "source": [
        "Verificando a versão do sistema operacional das máquinas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgDJdcdV3SJt"
      },
      "outputs": [],
      "source": [
        "value_counts = data['Census_OSSkuName'].value_counts(dropna=True, sort=True)\n",
        "df_val_counts = pd.DataFrame(value_counts)\n",
        "df_value_counts_reset = df_val_counts.reset_index()\n",
        "df_value_counts_reset.columns = ['unique_values', 'counts']\n",
        "new_data = data.loc[data['Census_OSSkuName'].isin(df_value_counts_reset.unique_values[:5])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L6UIeCI5n_E"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
        "fig.suptitle('Quantidade de Infeções por Versão do SO')\n",
        "\n",
        "sns.barplot(ax=ax[0], x='counts',y='unique_values',data=df_value_counts_reset[:20])\n",
        "sns.countplot(ax=ax[1], y='Census_OSSkuName', hue='HasDetections',data=new_data)\n",
        "\n",
        "ax[0].set_title(\"Quantidade de amostras por versão do SO\")\n",
        "ax[1].set_title(\"Infecção por versão do SO\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UH5MlqNXIxt"
      },
      "source": [
        "## Encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zUbC8KNYljf"
      },
      "source": [
        "**Abordagem 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2lU1Hv_VOJD"
      },
      "outputs": [],
      "source": [
        "#Transforma os valores das colunas categoricas em int\n",
        "def encodingCategoricalData(train, test):\n",
        "  enc = OrdinalEncoder()\n",
        "  for col in train.columns:\n",
        "    data = np.array(train[col]).reshape(-1,1)\n",
        "    data = np.append(data, np.array(test[col]).reshape(-1,1), axis = 0)\n",
        "    enc = enc.fit(data)\n",
        "    del data\n",
        "    gc.collect()\n",
        "    \n",
        "    train[col] = enc.transform(np.array(train[col]).reshape(-1,1))\n",
        "    test[col] = enc.transform(np.array(test[col]).reshape(-1,1))\n",
        "  \n",
        "  return train, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWLDsa55YpQP"
      },
      "source": [
        "**Abordagem 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp2y-lhahQ-a"
      },
      "outputs": [],
      "source": [
        "def encode_FE(df, col):\n",
        "  #realiza o enconding de frequencia da coluna, com base apenas em um dos datasets\n",
        "  vc = df[col].value_counts(dropna=False, normalize=True).to_dict()\n",
        "  nm = col+'_FE'\n",
        "  df[nm] = df[col].map(vc)\n",
        "  df[nm] = df[nm].astype('float32')\n",
        "  return [nm]\n",
        "\n",
        "def encode_FE2(df1, df2, col):\n",
        "  #realiza o enconding de frequencia da coluna, com base nos dados de treino e teste\n",
        "  df = pd.concat([df1[col],df2[col]])\n",
        "  vc = df.value_counts(dropna=False, normalize=True).to_dict()\n",
        "  nm = col+'_FE2'\n",
        "  df1[nm] = df1[col].map(vc)\n",
        "  df1[nm] = df1[nm].astype('float32')\n",
        "  df2[nm] = df2[col].map(vc)\n",
        "  df2[nm] = df2[nm].astype('float32')\n",
        "  return [nm]\n",
        "\n",
        "def factor_data(df_train, df_test, col):\n",
        "  # Reserva o valor 0, tornando a menor label 1\n",
        "  # Tranforma os valores NAN no valor mais alto do atributo\n",
        "  df_comb = pd.concat([df_train[col],df_test[col]],axis=0)\n",
        "  df_comb,_ = df_comb.factorize(sort=True)\n",
        "  df_comb += 1\n",
        "  \n",
        "  df_comb = np.where(df_comb==0, df_comb.max()+1, df_comb)\n",
        "  df_train[col] = df_comb[:len(df_train)]\n",
        "  df_test[col] = df_comb[len(df_train):]\n",
        "  del df_comb\n",
        "\n",
        "def relax_data(df_train, df_test, col):\n",
        "  #reduz a cardinalidade dos dados, simplificando o processo de treinamento\n",
        "  cv1 = pd.DataFrame(df_train[col].value_counts().reset_index().rename({col:'train'},axis=1))\n",
        "  cv2 = pd.DataFrame(df_test[col].value_counts().reset_index().rename({col:'test'},axis=1))\n",
        "  cv3 = pd.merge(cv1,cv2,on='index',how='outer')\n",
        "  factor = len(df_test)/len(df_train)\n",
        "  cv3['train'].fillna(0,inplace=True)\n",
        "  cv3['test'].fillna(0,inplace=True)\n",
        "  cv3['remove'] = False\n",
        "  cv3['remove'] = cv3['remove'] | (cv3['train'] < len(df_train)/10000)\n",
        "  cv3['remove'] = cv3['remove'] | (factor*cv3['train'] < cv3['test']/3)\n",
        "  cv3['remove'] = cv3['remove'] | (factor*cv3['train'] > 3*cv3['test'])\n",
        "  cv3['new'] = cv3.apply(lambda x: x['index'] if x['remove']==False else 0,axis=1)\n",
        "  cv3['new'],_ = cv3['new'].factorize(sort=True)\n",
        "  cv3.set_index('index',inplace=True)\n",
        "  cc = cv3['new'].to_dict()\n",
        "  df_train[col] = df_train[col].map(cc)\n",
        "  df_test[col] = df_test[col].map(cc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGjVUam90lmJ"
      },
      "source": [
        "## Engenharia de Características"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHoi1EcUEdc8"
      },
      "source": [
        "### Limpeza dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb0xhRtGryVn"
      },
      "outputs": [],
      "source": [
        "removed_cols = []\n",
        "good_cols = list(data_train.columns)\n",
        "\n",
        "for col in data_train.columns:    \n",
        "    # remove as colunas com alta taxa de valores nulos\n",
        "    na_rate = data_train[col].isnull().sum() / data_train.shape[0]\n",
        "    \n",
        "    # remove colunas com valores não balanceadas\n",
        "    unbalanced_rate = data_train[col].value_counts(normalize=True, dropna=False).values[0]\n",
        "    \n",
        "    if na_rate > na_rate_threshold:\n",
        "        good_cols.remove(col)\n",
        "        removed_cols.append(col)\n",
        "    elif unbalanced_rate > unbalanced_feature_rate_threshold:\n",
        "        good_cols.remove(col)\n",
        "        removed_cols.append(col)\n",
        "\n",
        "data_train = data_train[good_cols]\n",
        "\n",
        "good_cols.remove('HasDetections')\n",
        "data_test = data_test[good_cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvmVy75eX2Rz"
      },
      "source": [
        "### Ajustes dos Atributos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GL2ijzjRvUn"
      },
      "outputs": [],
      "source": [
        "#fazendo ajuste de uma das categorias do problema, para que não haja perdas significativas\n",
        "#apenas no arquivo de treino\n",
        "if 5244810 in data_train.index:\n",
        "  data_train.loc[5244810,'AvSigVersion'] = '1.273.1144.0'\n",
        "  data_train['AvSigVersion'].cat.remove_categories('1.2&#x17;3.1144.0',inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6LulIM8VIz_"
      },
      "outputs": [],
      "source": [
        "#Fazendo o ajuste do AppVersion, pois somente os 3 primeiros valores são importantes para o nosso modelo\n",
        "data_train['AppVersion2'] = data_train['AppVersion'].map(lambda x: np.int(x.split('.')[1]))\n",
        "data_test['AppVersion2'] = data_test['AppVersion'].map(lambda x: np.int(x.split('.')[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LVDfsVSWusV"
      },
      "outputs": [],
      "source": [
        "#criando uma informação acerca dos discos rigidos da máquina, pois usuários que sabem manipular melhor tais hardwares tem menos infecções na máquina\n",
        "data_train['driveA'] = data_train['Census_SystemVolumeTotalCapacity']/data_train['Census_PrimaryDiskTotalCapacity']\n",
        "data_test['driveA'] = data_test['Census_SystemVolumeTotalCapacity']/data_test['Census_PrimaryDiskTotalCapacity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZKY_lgMXT_O"
      },
      "outputs": [],
      "source": [
        "#representa o manejo do disco para o espaço alocado para o sistema operacional. Valendo o mesmo principio do DriveA\n",
        "data_train['driveB'] = data_train['Census_PrimaryDiskTotalCapacity'] - data_train['Census_SystemVolumeTotalCapacity']\n",
        "data_test['driveB'] = data_test['Census_PrimaryDiskTotalCapacity'] - data_test['Census_SystemVolumeTotalCapacity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biJlVevKX3DW"
      },
      "outputs": [],
      "source": [
        "#mapeando para timestamp das definições de antivirus do sistema\n",
        "AS_dates = np.load(os.path.join(DATA_ENV, 'AvSigVersionTimestamps.npy'), allow_pickle=True)[()]\n",
        "data_train['DateAS'] = data_train['AvSigVersion'].map(AS_dates)\n",
        "data_test['DateAS'] = data_test['AvSigVersion'].map(AS_dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld2C8MzsYC4C"
      },
      "outputs": [],
      "source": [
        "#mapeando para timestamp das definições de atualizações do sistema\n",
        "OS_dates = np.load(os.path.join(DATA_ENV, 'OSVersionTimestamps.npy'), allow_pickle=True)[()]\n",
        "data_train['DateOS'] = data_train['Census_OSVersion'].map(OS_dates)\n",
        "data_test['DateOS'] = data_test['Census_OSVersion'].map(OS_dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZJLz1zOZEhd"
      },
      "outputs": [],
      "source": [
        "#Variaveis que indicam quando o Windows Defender está desatualizado, comparando com a última data de atualização do sistema operacional.\n",
        "#Quando ele encontra-se desatualizado, significa que o usuário tem uma versão melhor de antivirus, ou não utiliza mais a máquina\n",
        "#Neste caso, apresentam menor quantidade de infecções.\n",
        "data_train['Lag1'] = data_train['DateAS'] - data_train['DateOS']\n",
        "data_train['Lag1'] = data_train['Lag1'].map(lambda x: x.days//7)\n",
        "data_test['Lag1'] = data_test['DateAS'] - data_test['DateOS']\n",
        "data_test['Lag1'] = data_test['Lag1'].map(lambda x: x.days//7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK3CHEVwZSVs"
      },
      "outputs": [],
      "source": [
        "#Faz o cálculo de tempo entre as informações de AvSigVersion, para saber se o antivirus encontra-se desatualizado. \n",
        "#Leva como base 26 de junhp de 2018 para o treino e 27 de setembro de 2018 para o teste\n",
        "data_train['Lag5'] = datetime(2018,7,26) - data_train['DateAS']\n",
        "data_train['Lag5'] = data_train['Lag5'].map(lambda x: x.days//1)\n",
        "data_train.loc[ data_train['Lag5']<0, 'Lag5' ] = 0\n",
        "\n",
        "data_test['Lag5'] = datetime(2018,9,27) - data_test['DateAS'] #PUBLIC TEST\n",
        "data_test['Lag5'] = data_test['Lag5'].map(lambda x: x.days//1)\n",
        "data_test.loc[data_test['Lag5']<0, 'Lag5'] = 0\n",
        "\n",
        "data_train['Lag5'] = data_train['Lag5'].astype('float32') #allow for NAN\n",
        "data_test['Lag5'] = data_test['Lag5'].astype('float32') #allow for NAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a03CSGM8aK00"
      },
      "outputs": [],
      "source": [
        "#salvando memória\n",
        "del data_train['DateAS'], data_train['DateOS']\n",
        "del data_test['DateAS'], data_test['DateOS']\n",
        "del AS_dates, OS_dates\n",
        "x=gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYm9pfBrm8j_"
      },
      "outputs": [],
      "source": [
        "#Quantidade de amostras por classe\n",
        "data_train['HasDetections'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liuf0xFBhq_T"
      },
      "source": [
        "### Encoding dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_C2TMVrYf5C"
      },
      "outputs": [],
      "source": [
        "cols6=['Lag1']\n",
        "cols8=['Lag5','driveB','driveA']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7K5TUo7htrE"
      },
      "outputs": [],
      "source": [
        "cols3 = []\n",
        "\n",
        "FE = ['Census_OSVersion', 'Census_OSBuildRevision', 'Census_InternalBatteryNumberOfCharges', 'AvSigVersion', 'Lag1']\n",
        "for col in FE:\n",
        "  cols3 += encode_FE(data_train, col)\n",
        "  encode_FE(data_test, col)\n",
        "\n",
        "FE2 = ['CountryIdentifier', 'Census_InternalBatteryNumberOfCharges']\n",
        "for col in FE2:\n",
        "  cols3 += encode_FE2(data_train, data_test, col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ7TPwGci6eq"
      },
      "outputs": [],
      "source": [
        "cols2 = ['CountryIdentifier', 'SkuEdition', 'Census_ProcessorCoreCount', 'Census_OSUILocaleIdentifier']\n",
        "cols = [x for x in data_train.columns if x not in ['HasDetections']+cols2+cols3+cols6+cols8]\n",
        "    \n",
        "for col in cols.copy():\n",
        "  rate = data_train[col].value_counts(normalize=True, dropna=False).values[0]\n",
        "  if rate > 0.98:\n",
        "    del data_train[col]\n",
        "    del data_test[col]\n",
        "    cols.remove(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nynmr4J150X8"
      },
      "outputs": [],
      "source": [
        "#colunas que devem ser removidas\n",
        "rmv=['Census_OSSkuName', 'Census_OSInstallLanguageIdentifier', 'SMode']\n",
        "for col in rmv:\n",
        "  try:\n",
        "    del data_train[col]\n",
        "    del data_test[col]\n",
        "    cols.remove(col)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "t=gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8hYmkVt6dT8"
      },
      "outputs": [],
      "source": [
        "for col in cols+cols2+cols6:\n",
        "  factor_data(data_train, data_test, col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39h9WrRI7DyG"
      },
      "outputs": [],
      "source": [
        "for col in cols+cols2:\n",
        "  relax_data(data_train, data_test, col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aH1mfCc84zMC"
      },
      "outputs": [],
      "source": [
        "data_train = data_train.drop(columns=['MachineIdentifier'])\n",
        "data_test = data_test.drop(columns=['MachineIdentifier'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rd5mdlv6ef8"
      },
      "outputs": [],
      "source": [
        "data_train, labels = data_train.drop(columns=['HasDetections']), data_train.HasDetections\n",
        "data_train['HasDetections'] = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwAOeiOr01Xn"
      },
      "outputs": [],
      "source": [
        "data_train['HasDetections'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn9nH6-qRljZ"
      },
      "outputs": [],
      "source": [
        "d = data_train.to_numpy()\n",
        "np.save(os.path.join(DATA_ENV, 'clean_train_data.npy'), d)\n",
        "del d\n",
        "t=gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbw2B_eIWlT2"
      },
      "outputs": [],
      "source": [
        "d = data_test.to_numpy()\n",
        "np.save(os.path.join(DATA_ENV, 'clean_test_data.npy'), d)\n",
        "del d\n",
        "t=gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O863EBC1wmkt"
      },
      "source": [
        "## Busca por Outliers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq0zmzLIAHuU"
      },
      "source": [
        "### Análise da Distribuição dos Dados Não Categóricos\n",
        "\n",
        "Começamos pela analise dos dados não categóricos, com o intuito de planejarmos a estratégia para busca e remoção dos outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsQnhVFmsRyO"
      },
      "outputs": [],
      "source": [
        "non_categorical_data = data.select_dtypes(exclude=['category'])\n",
        "len(non_categorical_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNPvDHid5ZY9",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "for i in range(0,len(non_categorical_data.columns),1):\n",
        "  fig, ax = plt.subplots()\n",
        "  col = non_categorical_data[non_categorical_data.columns[i]]\n",
        "  col.rename=non_categorical_data.columns[i]\n",
        "  k = \"{:.2f}\".format(kurtosis(col.dropna()))\n",
        "  s = \"{:.2f}\".format(skew(col.dropna()))\n",
        "  plt.xlabel('Bins')\n",
        "  plt.ylabel('Porcentagem')\n",
        "  plt.title('Histograma - '+col.name)\n",
        "  plt.text(0.6, .7, 'kur='+k+' skew='+s, bbox=dict(facecolor='white', alpha=0.7),fontsize='medium', transform=fig.transFigure)\n",
        "  hist = col.hist()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xragk0bbAHuU"
      },
      "source": [
        "Através do plot das distribuições, percebeu-se que a aplicação de técnicas mais simples para remoção de outliers não seria ideal. Portanto, fomos através de outras técnicas que fosse aplicavél a está etapa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrl1L1IhXni8"
      },
      "source": [
        "### HBOS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud0SYqntscSP"
      },
      "source": [
        "**Abordagem 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kERsqVci7r2"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = data_train.drop(columns=['HasDetections', 'MachineIdentifier']), data_train.HasDetections\n",
        "X_test = data_test.drop(columns=['MachineIdentifier'])\n",
        "\n",
        "#salvando memória, pois a base é bem custosa\n",
        "del data_train\n",
        "del data_test\n",
        "t=gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmF6hN3WElrF"
      },
      "outputs": [],
      "source": [
        "#Encoding das colunas categoricas\n",
        "train_enc, test_enc = encodingCategoricalData(\n",
        "    X_train.select_dtypes(exclude=['float16', 'float32', 'int8', 'int16', 'int32']),\n",
        "    X_test.select_dtypes(exclude=['float16', 'float32', 'int8', 'int16', 'int32']),\n",
        ")\n",
        "for col in train_enc.columns:\n",
        "  X_train[col] = train_enc[col]\n",
        "  X_test[col] = test_enc[col]\n",
        "\n",
        "#salvando memória, pois a base é bem custosa\n",
        "del train_enc\n",
        "del test_enc\n",
        "\n",
        "t=gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fds1wNb1nH-o"
      },
      "outputs": [],
      "source": [
        "np.save(\n",
        "  os.path.join(DATA_ENV, 'encoded_train.npy'), \n",
        "  np.append(X_train, y_train[:,None], axis=1)\n",
        ")\n",
        "np.save(os.path.join(DATA_ENV, 'encoded_test.npy'), X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l3wLXDYsYmr"
      },
      "source": [
        "**Abordagem 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvPF_Nydbvts"
      },
      "outputs": [],
      "source": [
        "data = np.load(os.path.join(DATA_ENV, 'clean_train_data.npy'))\n",
        "y_pos = data.shape[1]-1\n",
        "X_train = data[:, :y_pos]\n",
        "y_train = data[:, y_pos] \n",
        "\n",
        "del data\n",
        "t=gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8P5ju67kqu-V"
      },
      "outputs": [],
      "source": [
        "X_train = np.nan_to_num(X_train, nan=-1) #replace NAN com -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUkOuII9pIm6"
      },
      "outputs": [],
      "source": [
        "hbos_model = load(os.path.join(MODELS_ENV, 'hbos_model.joblib'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VN4wlnKqYY0r"
      },
      "outputs": [],
      "source": [
        "hbos_model = HBOS(n_bins=10, alpha=0.1, tol=0.5, contamination=0.1)\n",
        "hbos_model.fit(X_train)\n",
        "dump(hbos_model, os.path.join(MODELS_ENV, 'hbos_model3.joblib'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4xA2hDehhXi"
      },
      "outputs": [],
      "source": [
        "y_train_pred = hbos_model.labels_  #(0: inliers, 1: outliers)\n",
        "# y_train_scores = hbos_model.decision_scores_  #raw outlier scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4lTy-a6hizg"
      },
      "outputs": [],
      "source": [
        "(y_train_pred == 1).sum() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2jDJqHOhmKT"
      },
      "outputs": [],
      "source": [
        "X_train = X_train[y_train_pred == 0]\n",
        "y_train = y_train[y_train_pred == 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srQeXvk6ll9v"
      },
      "outputs": [],
      "source": [
        "X_train = np.append(X_train, y_train[:,None], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTMePBuklndu"
      },
      "outputs": [],
      "source": [
        "np.save(os.path.join(DATA_ENV, 'non_outliers_data_2.npy'), X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3eg3pXe6awV"
      },
      "source": [
        "## Classificadores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVc0PA9Z-4bH"
      },
      "source": [
        "### Teste Rápido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRhmTu9rAHuW"
      },
      "outputs": [],
      "source": [
        "data = np.load(os.path.join(DATA_ENV, \"non_outliers_data.npy\"))\n",
        "y_pos = data.shape[1]-1\n",
        "labels = data[:, y_pos] \n",
        "data = data[:, :y_pos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raLO1V_UAHuW"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(data, labels, test_size=0.3, random_state=12345)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4XFS_n0taxI"
      },
      "outputs": [],
      "source": [
        "del data\n",
        "del labels\n",
        "t=gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFZUdwXj-X3r"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "  'num_leaves': 60,\n",
        "  'min_data_in_leaf': 100, \n",
        "  'objective':'binary',\n",
        "  'max_depth': -1,\n",
        "  'learning_rate': 0.1,\n",
        "  \"boosting\": \"gbdt\",\n",
        "  \"feature_fraction\": 0.8,\n",
        "  \"bagging_freq\": 1,\n",
        "  \"bagging_fraction\": 0.8 ,\n",
        "  \"bagging_seed\": 42,\n",
        "  \"metric\": 'auc',\n",
        "  \"lambda_l1\": 0.1,\n",
        "  \"random_state\": 12345,\n",
        "  \"verbosity\": -1\n",
        "}\n",
        "\n",
        "lgb_train = lgb.Dataset(X_train[:200, :], label=Y_train[:200])\n",
        "lgb_val = lgb.Dataset(X_val[:200, :], label=Y_val[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOSqffxW-nFP"
      },
      "outputs": [],
      "source": [
        "evals_result = {}\n",
        "\n",
        "model = lgb.train(\n",
        "  params, \n",
        "  lgb_train, \n",
        "  4000,\n",
        "  valid_sets=[lgb_train, lgb_val], \n",
        "  early_stopping_rounds=200, \n",
        "  verbose_eval=100,\n",
        "  evals_result=evals_result\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epXdup0g-wwO"
      },
      "outputs": [],
      "source": [
        "model_name=\"lgb_model_t3.txt\"\n",
        "model.save_model(os.path.join(MODELS_ENV, model_name), num_iteration=model.best_iteration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N9CDvBD--L_"
      },
      "source": [
        "### Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCY0Dq-oJJ2z"
      },
      "outputs": [],
      "source": [
        "data = np.load(os.path.join(DATA_ENV, \"non_outliers_data.npy\"))\n",
        "y_pos = data.shape[1]-1\n",
        "labels = data[:, y_pos] \n",
        "data = data[:, :y_pos]\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(data, labels, test_size=0.3, random_state=12345)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb4B60kP_AUJ"
      },
      "outputs": [],
      "source": [
        "# classifiers\n",
        "classifiers = [\n",
        "  {\n",
        "  'clf': [SVC()],\n",
        "  'parameters': {\n",
        "    'clf__kernel':('linear', 'rbf'),\n",
        "    'clf__C':[1, 10]\n",
        "  }\n",
        "  },\n",
        "  {   \n",
        "    'clf': [RandomForestClassifier()],\n",
        "    'parameters': {\n",
        "      'clf__n_estimators': [200, 500],\n",
        "      'clf__max_features': ['auto', 'sqrt', 'log2'],\n",
        "      'clf__max_depth' : [4,5,6,7,8],\n",
        "      'clf__criterion' :['gini', 'entropy']\n",
        "    }\n",
        "  },\n",
        "  {\n",
        "    'clf': [DecisionTreeClassifier()],\n",
        "    'parameters': {\n",
        "      'clf__criterion': ['entropy', 'gini'],\n",
        "      'clf__max_depth': range(1,10),\n",
        "      'clf__min_samples_leaf': range(1,5),\n",
        "      'clf__min_samples_split': range(1,10)\n",
        "    }\n",
        "  }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V4ghGn6H_D5D"
      },
      "outputs": [],
      "source": [
        "result = []\n",
        "\n",
        "for classifier in classifiers:\n",
        "  # classificador\n",
        "  clf = classifier['clf'][0]\n",
        "\n",
        "  # obtendo argumentos do classificador\n",
        "  params = classifier['parameters']\n",
        "\n",
        "  #pipeline\n",
        "  step = [('clf', clf)]\n",
        "\n",
        "  # GridSearchCV, usando validação cruzada\n",
        "  search = GridSearchCV(Pipeline(step), param_grid=params, cv=5, verbose=1, n_jobs=-1)\n",
        "  search.fit(X_train, Y_train)\n",
        "\n",
        "  # armazenando os resultados\n",
        "  result.append(\n",
        "    {\n",
        "      'search': search,\n",
        "      'classifier': search.best_estimator_,\n",
        "      'best score': search.best_score_,\n",
        "      'best params': search.best_params_,\n",
        "      'cv': search.cv\n",
        "    }\n",
        "  )\n",
        "  print(result)\n",
        "  gc.collect()\n",
        "  \n",
        "# ordenando o resultado pela melhor pontuação\n",
        "result = sorted(result, key=itemgetter('best score'), reverse=True)\n",
        "\n",
        "# salvando o melhor classificador\n",
        "grid = result[0]['search']\n",
        "joblib.dump(grid, 'classifier.pickle')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9E4fgKhQrrt"
      },
      "source": [
        "### Hyperopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pnmph0SMxKyT"
      },
      "outputs": [],
      "source": [
        "data = np.load(os.path.join(DATA_ENV, \"non_outliers_data.npy\"))\n",
        "y_pos = data.shape[1]-1\n",
        "labels = data[:, y_pos] \n",
        "data = data[:, :y_pos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaZduVJwxKyU"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(data, labels, test_size=0.3, random_state=12345)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIU3EAxDLppv"
      },
      "outputs": [],
      "source": [
        "del data\n",
        "del labels\n",
        "t=gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8aMTvF_vyzf"
      },
      "outputs": [],
      "source": [
        "SPACE={\n",
        "  'num_leaves':hp.choice('num_leaves', [30, 60, 120, 512, 2048]),\n",
        "  'min_data_in_leaf':hp.choice('min_data_in_leaf', [50, 100, 200, 400]),\n",
        "  'max_depth': hp.choice('max_depth', [-1, 30, 60]),\n",
        "  'learning_rate': hp.choice('learning_rate', [0.001, 0.01, 0.02, 0.04, 0.1]),\n",
        "  'bagging_fraction': hp.choice('bagging_fraction', [0.7, 0.8, 0.9]),\n",
        "  'boosting': hp.choice('boosting', ['gbdt', 'rf', 'dart']),\n",
        "  'feature_fraction': hp.choice('feature_fraction', [0.7, 0.8, 0.9]),\n",
        "  # 'metric': hp.choice('metric', ['auc', 'mae', 'mse', 'binary_logloss'])\n",
        "  'metric': hp.choice('metric', ['auc'])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAtDpldNQuJr"
      },
      "outputs": [],
      "source": [
        "def train_lgbm(hyper_space):\n",
        "  #mae=l1\n",
        "  #mse=l2\n",
        "  #binary_logloss=binary_logloss\n",
        "\n",
        "  evals_result = {}\n",
        "  params = {\n",
        "    'num_leaves': hyper_space['num_leaves'],\n",
        "    'min_data_in_leaf': hyper_space['min_data_in_leaf'], \n",
        "    'objective':'binary',\n",
        "    'max_depth': hyper_space['max_depth'],\n",
        "    'learning_rate': hyper_space['learning_rate'],\n",
        "    \"boosting\": hyper_space['boosting'],\n",
        "    \"feature_fraction\": hyper_space['feature_fraction'],\n",
        "    \"bagging_freq\": 1,\n",
        "    \"bagging_fraction\": hyper_space['bagging_fraction'],\n",
        "    \"bagging_seed\": 42,\n",
        "    \"metric\": hyper_space['metric'],\n",
        "    \"lambda_l1\": 0.1,\n",
        "    \"random_state\": 12345,\n",
        "    \"verbosity\": -1\n",
        "  }\n",
        "\n",
        "  lgb_train = lgb.Dataset(X_train, label=Y_train)\n",
        "  lgb_val = lgb.Dataset(X_val, label=Y_val)\n",
        "\n",
        "  model = lgb.train(\n",
        "    params, \n",
        "    lgb_train, \n",
        "    4000,\n",
        "    valid_sets=[lgb_train, lgb_val], \n",
        "    early_stopping_rounds=200, \n",
        "    verbose_eval=100,\n",
        "    evals_result=evals_result,\n",
        "  )\n",
        "\n",
        "  if hyper_space['metric'] != 'mae' and hyper_space['metric'] != 'mse':\n",
        "    acc = evals_result['valid_1'][hyper_space['metric']][model.best_iteration]\n",
        "  elif hyper_space['metric'] == 'mae':\n",
        "    acc = evals_result['valid_1']['l1'][model.best_iteration]\n",
        "  else:\n",
        "    acc = evals_result['valid_1']['l2'][model.best_iteration]\n",
        "\n",
        "  return {\n",
        "      'loss': 1-acc,\n",
        "      'accuracy': acc,\n",
        "      'space': hyper_space,\n",
        "      'status': STATUS_OK\n",
        "  }\n",
        "\n",
        "def optmize_lgbm():  \n",
        "  try:\n",
        "    trials = pickle.load(open(os.path.join(DATA_ENV, \"results.pkl\"), \"rb\"))\n",
        "    print(\"Found saved Trials! Loading...\")\n",
        "    print(\"Rerunning from {} trials to add another one.\".format(len(trials.trials)))\n",
        "  except:\n",
        "    trials = Trials()\n",
        "    print(\"Starting from scratch: new trials.\")\n",
        "  \n",
        "  best = fmin(\n",
        "      train_lgbm,\n",
        "      SPACE,\n",
        "      algo=tpe.suggest,\n",
        "      trials=trials,\n",
        "      max_evals=20\n",
        "  )\n",
        "\n",
        "  best = space_eval(SPACE, best)\n",
        "  pickle.dump(trials, open(os.path.join(DATA_ENV, \"results.pkl\"), \"wb\"))\n",
        "  return best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r27iFUq1xqpH"
      },
      "outputs": [],
      "source": [
        "#Otimizando o modelo LGBM com algoritmo de TPE\n",
        "i=0\n",
        "best=None\n",
        "while i!=100:\n",
        "  print('Otimizando o modelo')\n",
        "  try:\n",
        "    best = optmize_lgbm()\n",
        "  except Exception as err:\n",
        "    err_str = str(err)\n",
        "    print(err_str)\n",
        "  i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-nvtSNFmRVu"
      },
      "source": [
        "## Criando Arquivo para Submissão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B783aKsB9tFR"
      },
      "outputs": [],
      "source": [
        "def make_submission_file(data_path, preds):\n",
        "  mac_id = pd.read_csv(os.path.join(data_path, 'test.csv'), dtype={'MachineIdentifier':'category'}, usecols=[\"MachineIdentifier\"])\n",
        "  mac_id[\"HasDetections\"] = preds\n",
        "  mac_id.to_csv(os.path.join(DATA_ENV, \"sample_submission_4.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v236JTyGMvBl"
      },
      "outputs": [],
      "source": [
        "model_name=\"lgb_model_t3.txt\"\n",
        "model = lgb.Booster(model_file=os.path.join(MODELS_ENV, model_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZC8driMhF_F"
      },
      "outputs": [],
      "source": [
        "data_test = np.load(os.path.join(DATA_ENV, \"clean_test_data.npy\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT4c1VXEZvyn"
      },
      "outputs": [],
      "source": [
        "data_test = np.nan_to_num(data_test, nan=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izi2zDnWiAcf"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGpmFqwT93C5"
      },
      "outputs": [],
      "source": [
        "make_submission_file(DATA_ENV, preds)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "w_IkQfpf7ly4",
        "6UH5MlqNXIxt",
        "mGjVUam90lmJ",
        "rHoi1EcUEdc8",
        "XvmVy75eX2Rz",
        "liuf0xFBhq_T",
        "O863EBC1wmkt",
        "Hq0zmzLIAHuU",
        "eVc0PA9Z-4bH",
        "6N9CDvBD--L_",
        "h-nvtSNFmRVu"
      ],
      "machine_shape": "hm",
      "name": "Microsoft_Malware_Prediction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "a25b86af9ffa28ce90e1e919a8b999ae4e1c9c690d44a9f9a8cdc24865d80da7"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}